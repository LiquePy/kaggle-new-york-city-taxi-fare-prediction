{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n%matplotlib inline \nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"types = {'fare_amount': 'float32',\n         'pickup_longitude': 'float32',\n         'pickup_latitude': 'float32',\n         'dropoff_longitude': 'float32',\n         'dropoff_latitude': 'float32',\n         'passenger_count': 'uint8'}\ncols = ['fare_amount', 'pickup_datetime', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count']\ndef load_train_data(Nrows):\n    return pd.read_csv('../input/train.csv', nrows=Nrows, dtype=types, usecols=cols, infer_datetime_format=True, parse_dates=[\"pickup_datetime\"]) # total nrows = 55423855","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39d045f03176f2427bf76f18c55e88ef8d6a4c63","collapsed":true},"cell_type":"code","source":"# Handling missing data and outliers; for feature engineering refer to the Pipe_line\ndef clean_missing_outliers(train_data):\n    print('shape of before FE: ', train_data.shape)\n    #handling missing data\n    train_data.dropna(inplace=True)\n    # removing outliers\n    train_data.drop(train_data.loc[(train_data.fare_amount<=0) | (train_data.fare_amount>60)].index, inplace=True)\n    train_data.drop(train_data.loc[(train_data.pickup_longitude<-74.03) | (train_data.pickup_longitude>-73.75)].index, inplace=True)\n    train_data.drop(train_data.loc[(train_data.dropoff_longitude<-74.03) | (train_data.dropoff_longitude>-73.75)].index, inplace=True)\n    train_data.drop(train_data.loc[(train_data.pickup_latitude<40.63) | (train_data.pickup_latitude>40.85)].index, inplace=True)\n    train_data.drop(train_data.loc[(train_data.dropoff_latitude<40.63) | (train_data.dropoff_latitude>40.85)].index, inplace=True)\n    train_data.drop(train_data.loc[(train_data.passenger_count>7)].index, inplace=True)\n\n    print('shape of after FE: ', train_data.shape)\n    return train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39d5247eb362afc99b33447f7560f683a72f0d3f","collapsed":true},"cell_type":"code","source":"# train test split\ndef tt_split(train_data):\n    from sklearn.model_selection import train_test_split\n    return train_test_split(train_data, test_size=0.2, random_state=23)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f396e3a73a76f51e3c7176039224afcdb4ba1c5","collapsed":true},"cell_type":"code","source":"# Get map_hour_dict & map_weekday_dict to be used in ModifyAttributes class\ndef get_mapping_dict(nrows=100, col='hour'):\n    train_data = load_train_data(nrows)\n    train_data = clean_missing_outliers(train_data)\n    # ATTENTION: REMOVE STANDARDIZATION FROM THE PIPLELINE BEFORE RUNNING THIS FUNCTION\n    train_data_tr, train_data_labels, pipeline, final_cols = Pipe_line(train_data)\n    assembled_df = pd.DataFrame(np.c_[train_data_labels, train_data_tr], columns=['fare_amount'] + final_cols)\n    assembled_df = assembled_df[assembled_df.distance_km >= 0.1]\n    assembled_df.drop(assembled_df[(assembled_df.distance_km <= 0.5)].index, inplace=True)\n    assembled_df['fare_per_km'] = assembled_df['fare_amount']/assembled_df['distance_km']\n    summary_hour_duration = pd.DataFrame(assembled_df.astype(dtype='int32').groupby([col, 'year'])['fare_per_km'].mean())\n    summary_hour_duration['unit']=1\n    summary_hour_duration.reset_index(inplace=True)\n    \n    sns.tsplot(data=summary_hour_duration, time=col, unit = \"unit\", condition=\"year\", value=\"fare_per_km\")\n    \n    summary_hour_duration2 = summary_hour_duration.astype(dtype='int32').groupby([col])['fare_per_km'].mean()\n    summary_hour_duration2.sort_values(ascending=True, inplace=True)\n    print(summary_hour_duration2)\n    output_dict = dict(zip(summary_hour_duration2.index, range(0, 25)))\n    print(output_dict)\n\n#get_mapping_dict(5000000, col='hour')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4fd76dece56dd30fd0235ca725e314a3e5ae0fa","collapsed":true},"cell_type":"code","source":"# Transformation part of the Pipe_line given a block below\nfrom sklearn.base import BaseEstimator, TransformerMixin\ntimeix, lat1ix, lon1ix, lat2ix, lon2ix = 0, 2, 1, 4, 3\n\n\nclass ModifyAttributes(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def distance(self, lat1, lon1, lat2, lon2):\n        p = 0.017453292519943295 # Pi/180\n        a = 0.5 - np.cos((lat2 - lat1) * p)/2 + np.cos(lat1 * p) * np.cos(lat2 * p) * (1 - np.cos((lon2 - lon1) * p)) / 2\n        return 12742 * np.arcsin(np.sqrt(a)) # 2*R*asin...\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n    #Feature engineering\n        distance_km = [self.distance(X[_, lat1ix], X[_, lon1ix], X[_, lat2ix], X[_, lon2ix]) for _ in range(X.shape[0])]\n        year = [_.year for _ in X[:, timeix]]\n        month = [_.month for _ in X[:, timeix]]\n        hour = [_.hour for _ in X[:, timeix]]\n        map_hour_dict = {5: 0, 3: 1, 4: 2, 6: 3, 1: 4, 2: 5, 0: 6, 21: 7, 20: 8, 22: 9, 23: 10, 7: 11, 9: 12, 12: 13, 13: 14, 14: 15, 15: 16, 16: 17, 17: 18, 18: 19, 19: 20, 8: 21, 10: 22, 11: 23}\n        mapped_hour = [map_hour_dict[_.hour] for _ in X[:, timeix]]\n        map_weekday_dict = {6: 0, 5: 1, 0: 2, 1: 3, 2: 4, 3: 5, 4: 6}\n        weekday = [_.weekday() for _ in X[:, timeix]]\n        mapped_weekday = [map_weekday_dict[_.weekday()] for _ in X[:, timeix]]\n    #     train_data['is_weekend'] = np.where(train_data.weekday<5, 0, 1)\n    #     train_data['is_rush_hour'] = np.where((train_data.hour<8)  | (train_data.hour > 18), 0, 1)\n        return np.c_[X[:, 1:], distance_km, year, month, hour, mapped_hour, weekday, mapped_weekday]\n\n#train_data.drop(train_data.loc[(train_data.distance_km<=0.1)].index, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad138685c15e64716956cb1cc5074a39cbcdf827","collapsed":true},"cell_type":"code","source":"# SKlearn's pipeline for dataset preparation\ndef Pipe_line(train_data):\n    train_data_labels = train_data.fare_amount.copy()\n    train_data = train_data.drop('fare_amount', axis=1)\n    from sklearn.pipeline import Pipeline\n    from sklearn.preprocessing import StandardScaler\n\n    pipeline = Pipeline([\n        ('attribs_adder', ModifyAttributes()),\n        ('std_scaler', StandardScaler())\n    ])\n\n    final_cols = list(train_data.columns[1:].values) + ['distance_km', 'year', 'month', 'hour', 'mapped_hour', 'weekday', 'mapped_weekday']\n\n    train_data_tr = pipeline.fit_transform(train_data.values)\n    train_data_tr = pd.DataFrame(train_data_tr, columns=final_cols)\n    train_data_tr.head()\n    return train_data_tr, train_data_labels, pipeline, final_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2adc3d3fe544b8552145d8e770238552c72fea4","scrolled":true,"collapsed":true},"cell_type":"code","source":"# Get correlations - Pearson's r\ndef get_cors(nrows=100000):\n    train_data = load_train_data(nrows)\n    train_data = clean_missing_outliers(train_data)\n    train_data, test_set = tt_split(train_data)\n    train_data_tr, train_data_labels, pipeline, final_cols = Pipe_line(train_data)\n    train_data_tr = pd.DataFrame(data=train_data_tr, columns=final_cols)\n    \n    # Plot correlations\n    from scipy.stats import spearmanr\n    import warnings\n    warnings.filterwarnings(\"ignore\")\n\n    labels = []\n    values = []\n    for col in train_data_tr.columns:\n        labels.append(col)\n        values.append(spearmanr(train_data_tr[col].values, train_data_labels)[0])\n\n    corr_df = pd.DataFrame({'col_labels':labels, 'corr_values':values})\n    corr_df = corr_df.sort_values(by='corr_values')\n    \n    ind = np.arange(corr_df.shape[0])\n    width = 0.9\n    fig, ax = plt.subplots(figsize=(10,10))\n    rects = ax.barh(ind, np.array(corr_df.corr_values.values), color='red')\n    ax.set_yticks(ind)\n    ax.set_yticklabels(corr_df.col_labels.values, rotation='horizontal')\n    ax.set_xlabel(\"Correlation coefficient\")\n    ax.set_title(\"Correlation coefficient of the variables\")\n    plt.grid()\n    plt.show()\n\n#get_cors(5000000)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66ba1fa806dcf8b9315d70efbcb59c34ed6a21be"},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ce62f209c51b6bdee75d2f67e8bbc9bb1d3c0910"},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nKfolds = 5\n\ndef display_scores(scores):\n    print('Scores:', scores)\n    print('Mean RMSE:', scores.mean())\n    print('STD of RMSE:', scores.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5796f55ae834bf5b357eda374968ee71a3df8622","collapsed":true},"cell_type":"code","source":"# Linear Regression\ndef Lin_reg(train_data_tr, train_data_labels, cross_eval=False):\n    from sklearn.linear_model import LinearRegression\n    lin_reg = LinearRegression()\n    lin_reg.fit(train_data_tr, train_data_labels)\n    if cross_eval:\n        lin_scores = cross_val_score(lin_reg, train_data_tr, train_data_labels, scoring='neg_mean_squared_error', cv=Kfolds)\n        lin_rmse_scores = np.sqrt(-lin_scores)\n        display_scores(lin_rmse_scores)\n    return lin_reg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0dbec7ad2d4ad16b6ddc8a97137b69b13f30125","collapsed":true},"cell_type":"code","source":"# Decision Tree model\ndef Tree_reg(train_data_tr, train_data_labels, cross_eval=False):\n    from sklearn.tree import DecisionTreeRegressor\n    tree_reg = DecisionTreeRegressor()\n    tree_reg.fit(train_data_tr, train_data_labels)\n    if cross_eval:\n        tree_scores = cross_val_score(tree_reg, train_data_tr, train_data_labels, scoring='neg_mean_squared_error', cv=Kfolds)\n        tree_rmse_scores = np.sqrt(-tree_scores)\n        display_scores(tree_rmse_scores)\n    return tree_reg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33e1c98cb7954b0f7593ebc7c932283137c2cfee","collapsed":true},"cell_type":"code","source":"# XGBoost Regressor\ndef Xgb_reg(train_data_tr, train_data_labels, cross_eval=False):\n    from xgboost import XGBRegressor\n    xgb_reg = XGBRegressor(n_estimators=130, max_depth= 6, subsample=0.8, eta= 0.05, min_child_weight=10)\n    xgb_reg.fit(train_data_tr, train_data_labels)\n    if cross_eval:\n        xgb_scores = cross_val_score(xgb_reg, train_data_tr, train_data_labels, scoring='neg_mean_squared_error', cv=Kfolds)\n        xgb_rmse_scores = np.sqrt(-xgb_scores)\n        display_scores(xgb_rmse_scores)\n    return xgb_reg\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a1fd90abafdc1c467925e29406418383d0b3a3d","collapsed":true},"cell_type":"code","source":"# SGD Regressor\ndef Sgd_reg(train_data_tr, train_data_labels, cross_eval=False):\n    from sklearn.linear_model import SGDRegressor\n    sgd_reg = SGDRegressor()\n    sgd_reg.fit(train_data_tr, train_data_labels)\n    if cross_eval:\n        sgd_scores = cross_val_score(sgd_reg, train_data_tr, train_data_labels, scoring='neg_mean_squared_error', cv=Kfolds)\n        sgd_rmse_scores = np.sqrt(-sgd_scores)\n        display_scores(sgd_rmse_scores)\n    return sgd_reg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d7d21157665e9c729029c14d1faf7760b6f8d51","collapsed":true},"cell_type":"code","source":"# Random Forest model\ndef Forest_reg(train_data_tr, train_data_labels, cross_eval=False):\n    from sklearn.ensemble import RandomForestRegressor\n    forest_reg = RandomForestRegressor(n_estimators=130, max_features=5)\n    forest_reg.fit(train_data_tr, train_data_labels)\n    if cross_eval:\n        forest_scores = cross_val_score(forest_reg, train_data_tr, train_data_labels, scoring='neg_mean_squared_error', cv=Kfolds)\n        forest_rmse_scores = np.sqrt(-forest_scores)\n        display_scores(forest_rmse_scores)\n    return forest_reg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca2e36eae4679d0c3d2b9b66cc0b38afbe378520","collapsed":true},"cell_type":"code","source":"# Fine-tune random forest\n# from sklearn.model_selection import GridSearchCV\n# param_grid = [\n#     {'n_estimators': [3, 10, 30, 100], 'max_features': [2, 4, 6, 8]}\n# ]\n# grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error')\n# grid_search.fit(train_data_tr, train_data_labels)\n# grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5468307414121eb678746600363db81e1217d89","collapsed":true},"cell_type":"code","source":"# cvres = grid_search.cv_results_\n# for mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n#     print(np.sqrt(-mean_score), params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d98aee0fdbf48eb2e28bd71d4ffd66a7d85094e","collapsed":true},"cell_type":"code","source":"# Fine - tune eXtreme Gradient Boost\ndef fine_tune_xgb(nrows):\n    train_data = load_train_data(nrows)\n    train_data = clean_missing_outliers(train_data)\n    train_data, test_set = tt_split(train_data)\n    train_data_tr, train_data_labels, pipeline, final_cols = Pipe_line(train_data)\n    test_data_tr, test_data_labels, pipeline, final_cols = Pipe_line(test_set)\n    \n    import xgboost as xgb\n    FOREVER_COMPUTING_FLAG = False\n    xgb_pars = []\n    for MCW in [10, 20, 50, 75, 100]:\n        for ETA in [0.05, 0.1, 0.15]:\n            for CS in [0.3, 0.4, 0.5]:\n                for MD in [6, 8, 10, 12, 15]:\n                    for SS in [0.5, 0.6, 0.7, 0.8, 0.9]:\n                        for LAMBDA in [0.5, 1., 1.5,  2., 3.]:\n                            xgb_pars.append({'min_child_weight': MCW, 'eta': ETA, \n                                             'colsample_bytree': CS, 'max_depth': MD,\n                                             'subsample': SS, 'lambda': LAMBDA, \n                                             'nthread': -1, 'booster' : 'gbtree', 'eval_metric': 'rmse',\n                                             'silent': 1, 'objective': 'reg:linear'})\n\n    dtrain = xgb.DMatrix(train_data_tr, label=train_data_labels)\n    dvalid = xgb.DMatrix(test_data_tr, label=test_data_labels)\n    watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n\n    for i in range(100):\n        xgb_par = np.random.choice(xgb_pars, 1)[0]\n        print(xgb_par)\n        model = xgb.train(xgb_par, dtrain, 2000, watchlist, early_stopping_rounds=50,\n                          maximize=False, verbose_eval=100)\n        print('Modeling RMSE %.5f' % model.best_score)\n\n#fine_tune_xgb(nrows=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50f5300b491c7901a15f9e80eecf6e62757528b5","collapsed":true},"cell_type":"code","source":"# best_model = grid_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a70fa30c4a76df1446cef7df68eb05a90dc7083f"},"cell_type":"markdown","source":"# Evaluate on the Test Set"},{"metadata":{"trusted":true,"_uuid":"2094bc2e4f3121843d8d7240cb08429091aadcd0","scrolled":true,"collapsed":true},"cell_type":"code","source":"def evaluate_model(model, pipeline, test_set, final_cols):\n    X_test = test_set.drop(['fare_amount'], axis=1)\n    y_test = test_set.fare_amount.values\n    X_test_prepared = pipeline.transform(X_test.values)\n    X_test_prepared = pd.DataFrame(X_test_prepared, columns=final_cols)\n    final_predictions = model.predict(X_test_prepared)\n    final_mse = mean_squared_error(y_test, final_predictions)\n    final_rmse = np.sqrt(final_mse)\n    plt.scatter(y_test, final_predictions, alpha=0.3)\n    plt.xlabel('measured')\n    plt.ylabel('predicted')\n    plt.show()\n    fig = plt.figure()\n    res = stats.probplot((y_test - final_predictions), plot=plt)\n    plt.show()\n    print('RMSE on evaluation set: ', final_rmse)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b56b7ce050a5739a6ea0ab86e79385e432c286b"},"cell_type":"markdown","source":"# Launch! (predict test.csv)"},{"metadata":{"trusted":true,"_uuid":"bdd4f6763b915b36da807e4a296e09d3d8f54a30"},"cell_type":"code","source":"def write_test_file(model, nrows, model_name, pipeline, final_cols, train_data_labels, brute_force=False):\n    test_csv = pd.read_csv('../input/test.csv', dtype=types, infer_datetime_format=True, parse_dates=[\"pickup_datetime\"])\n    X_test_csv_keys = test_csv.key.copy()\n    X_test_csv = test_csv.drop('key', axis=1)\n    X_test_csv_prepared = pipeline.transform(X_test_csv.values)\n    X_test_csv_prepared = pd.DataFrame(X_test_csv_prepared, columns=final_cols)\n    final_predictions_csv = model.predict(X_test_csv_prepared)\n    print(final_predictions_csv)\n    # Apply brute force rules...\n    def brute_force_results(inp_data, train_data_labels):\n        q1 = train_data_labels.quantile(0.0042)\n        q2 = train_data_labels.quantile(0.99)\n        \n        dataframe = pd.DataFrame(inp_data)\n        dataframe[0] = dataframe[0].apply(lambda x: x if x > q1 else x*1.1)\n        dataframe[0] = dataframe[0].apply(lambda x: x if x < q2 else x*0.77)\n        return dataframe[0].values\n    \n    brt_str =''\n    if brute_force:\n        final_predictions_csv = brute_force_results(final_predictions_csv, train_data_labels)\n        brt_str = '_w_bf'\n    \n    submission = pd.DataFrame(data={'key':X_test_csv_keys, 'fare_amount': final_predictions_csv})\n    print(submission.head())\n    filename = 'submission_w_' + str(nrows) + '-training_w_' + str(model_name.__name__) + brt_str + '.csv'\n    submission.to_csv(filename, index=None)\n    print(filename, ' file is written.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"872a3bb42f8abeec06d729a7ee87cd792757f2dc","scrolled":true},"cell_type":"code","source":"import time\ndef run_entire_process(model=Lin_reg, cross_eval=False, test_eval=False, launch=False, nrows=1000, brute_force=False):\n    print('running ', str(model.__name__), ' on ', str(nrows), ' training rows with cross_eval=', str(cross_eval),\n         'test_eval=', str(test_eval), ' launch=', str(launch))\n    start_time = time.time()\n    train_data = load_train_data(nrows)\n    train_data = clean_missing_outliers(train_data)\n    train_data, test_set = tt_split(train_data)\n    train_data_tr, train_data_labels, pipeline, final_cols = Pipe_line(train_data)\n    modl = model(train_data_tr, train_data_labels, cross_eval)\n    if test_eval:\n        evaluate_model(modl, pipeline, test_set, final_cols)\n    if launch:\n        write_test_file(modl, nrows, model, pipeline, final_cols, train_data_labels, brute_force)\n    print('run time = ', time.time() - start_time, ' secs')\n    \nModels = [Lin_reg, Tree_reg, Xgb_reg, Sgd_reg, Forest_reg]\n# for _ in Models:\n#     run_entire_process(model=_, cross_eval=True, test_eval=True, launch=False, nrows=1000000)\nrun_entire_process(model=Models[2], cross_eval=True, test_eval=True, launch=True, nrows=1000000, brute_force=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"783b310a5363936f1d4fd0ef270544b8ccfc961a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}